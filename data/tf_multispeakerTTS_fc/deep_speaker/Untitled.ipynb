{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4417ea43",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p282_019\n",
      "p295_334\n",
      "p295_047\n",
      "p302_013\n",
      "p303_303\n",
      "p305_423\n",
      "p306_114\n",
      "p306_140\n",
      "p306_235\n",
      "p317_424\n",
      "p330_424\n",
      "p335_424\n",
      "p345_266\n",
      "p345_387\n",
      "p345_388\n",
      "p323_424\n",
      "p326_365\n",
      "p351_361\n",
      "p306_151\n",
      "p295_001\n",
      "p300_155\n",
      "p306_352\n",
      "p254_368\n",
      "p232_212\n",
      "p282_116\n",
      "p277_388\n",
      "p282_008\n",
      "p240_351\n",
      "p316_189\n",
      "p258_109\n",
      "p272_044\n",
      "p286_029\n",
      "p304_023\n",
      "p306_148\n",
      "p306_149\n",
      "p306_150\n",
      "p306_152\n",
      "p239_083\n"
     ]
    }
   ],
   "source": [
    "deleted_audio_files = [\n",
    "    \"p282_019\",\n",
    "    \"p295_334\",\n",
    "    \"p295_047\",\n",
    "    \"p302_013\",\n",
    "    \"p303_303\",\n",
    "    \"p305_423\",\n",
    "    \"p306_114\",\n",
    "    \"p306_140\",\n",
    "    \"p306_235\",\n",
    "    \"p317_424\",\n",
    "    \"p330_424\",\n",
    "    \"p335_424\",\n",
    "    \"p345_266\",\n",
    "    \"p345_387\",\n",
    "    \"p345_388\",\n",
    "    \"p323_424\",\n",
    "    \"p326_365\",\n",
    "    \"p351_361\",\n",
    "    \"p306_151\",\n",
    "    \"p295_001\",\n",
    "    \"p300_155\",\n",
    "    \"p306_352\",\n",
    "    \"p254_368\",\n",
    "    \"p232_212\",\n",
    "    \"p282_116\",\n",
    "    \"p277_388\",\n",
    "    \"p282_008\",\n",
    "    \"p240_351\",\n",
    "    \"p316_189\",\n",
    "    \"p258_109\",\n",
    "    \"p272_044\",\n",
    "    \"p286_029\",\n",
    "    \"p304_023\",\n",
    "    \"p306_148\",\n",
    "    \"p306_149\",\n",
    "    \"p306_150\",\n",
    "    \"p306_152\",\n",
    "    \"p239_083\"\n",
    "]\n",
    "\n",
    "# Example usage: print all file names\n",
    "for file_name in deleted_audio_files:\n",
    "    print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c367b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated metadata saved.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# List of deleted audio file identifiers (from previous example)\n",
    "deleted_audio_files = [\n",
    "    \"p282_019\",\n",
    "    \"p295_334\",\n",
    "    \"p295_047\",\n",
    "    \"p302_013\",\n",
    "    \"p303_303\",\n",
    "    \"p305_423\",\n",
    "    \"p306_114\",\n",
    "    \"p306_140\",\n",
    "    \"p306_235\",\n",
    "    \"p317_424\",\n",
    "    \"p330_424\",\n",
    "    \"p335_424\",\n",
    "    \"p345_266\",\n",
    "    \"p345_387\",\n",
    "    \"p345_388\",\n",
    "    \"p323_424\",\n",
    "    \"p326_365\",\n",
    "    \"p351_361\",\n",
    "    \"p306_151\",\n",
    "    \"p295_001\",\n",
    "    \"p300_155\",\n",
    "    \"p306_352\",\n",
    "    \"p254_368\",\n",
    "    \"p232_212\",\n",
    "    \"p282_116\",\n",
    "    \"p277_388\",\n",
    "    \"p282_008\",\n",
    "    \"p240_351\",\n",
    "    \"p316_189\",\n",
    "    \"p258_109\",\n",
    "    \"p272_044\",\n",
    "    \"p286_029\",\n",
    "    \"p304_023\",\n",
    "    \"p306_148\",\n",
    "    \"p306_149\",\n",
    "    \"p306_150\",\n",
    "    \"p306_152\",\n",
    "    \"p239_083\"\n",
    "]\n",
    "\n",
    "# Path to your existing metadata.csv file\n",
    "input_file_path = '/home/marinjezidzic/Downloads/tf_multispeakerTTS_fc/datasets/vctk/metadata.csv'\n",
    "\n",
    "# Path to the new (or the same to overwrite) metadata file\n",
    "output_file_path = '/home/marinjezidzic/Downloads/tf_multispeakerTTS_fc/datasets/vctk/updated_metadata.csv'\n",
    "\n",
    "# Open the existing metadata file for reading and the new file for writing\n",
    "with open(input_file_path, mode='r', encoding='utf-8') as infile, \\\n",
    "     open(output_file_path, mode='w', encoding='utf-8', newline='') as outfile:\n",
    "    \n",
    "    # Create a CSV reader and writer\n",
    "    reader = csv.reader(infile, delimiter='|')\n",
    "    writer = csv.writer(outfile, delimiter='|')\n",
    "    \n",
    "    # Iterate over each row in the existing metadata file\n",
    "    for row in reader:\n",
    "        # Check if the current row's file identifier is in the list of deleted files\n",
    "        if row[1] not in deleted_audio_files:\n",
    "            # If not, write the row to the new metadata file\n",
    "            writer.writerow(row)\n",
    "\n",
    "print(\"Updated metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d6acab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p225\n",
      "p226\n",
      "p227\n",
      "p228\n",
      "p229\n",
      "p230\n",
      "p231\n",
      "p232\n",
      "p233\n",
      "p234\n",
      "p236\n",
      "p237\n",
      "p238\n",
      "p239\n",
      "p240\n",
      "p241\n",
      "p243\n",
      "p244\n",
      "p245\n",
      "p246\n",
      "p247\n",
      "p248\n",
      "p249\n",
      "p250\n",
      "p251\n",
      "p252\n",
      "p253\n",
      "p254\n",
      "p255\n",
      "p256\n",
      "p257\n",
      "p258\n",
      "p259\n",
      "p260\n",
      "p261\n",
      "p262\n",
      "p263\n",
      "p264\n",
      "p265\n",
      "p266\n",
      "p267\n",
      "p268\n",
      "p269\n",
      "p270\n",
      "p271\n",
      "p272\n",
      "p273\n",
      "p274\n",
      "p275\n",
      "p276\n",
      "p277\n",
      "p278\n",
      "p279\n",
      "p280\n",
      "p281\n",
      "p282\n",
      "p283\n",
      "p284\n",
      "p285\n",
      "p286\n",
      "p287\n",
      "p288\n",
      "p292\n",
      "p293\n",
      "p294\n",
      "p295\n",
      "p297\n",
      "p298\n",
      "p299\n",
      "p300\n",
      "p301\n",
      "p302\n",
      "p303\n",
      "p304\n",
      "p305\n",
      "p306\n",
      "p307\n",
      "p308\n",
      "p310\n",
      "p311\n",
      "p312\n",
      "p313\n",
      "p314\n",
      "p316\n",
      "p317\n",
      "p318\n",
      "p323\n",
      "p326\n",
      "p329\n",
      "p330\n",
      "p333\n",
      "p334\n",
      "p335\n",
      "p336\n",
      "p339\n",
      "p340\n",
      "p341\n",
      "p343\n",
      "p345\n",
      "p347\n",
      "p351\n",
      "p374\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Specify the path to the folder containing the files\n",
    "folder_path = '/home/marinjezidzic/Downloads/tf_multispeakerTTS_fc/datasets/vctk/synthesizer/embeds'\n",
    "\n",
    "# Initialize an empty set to store unique prefixes\n",
    "unique_prefixes = set()\n",
    "\n",
    "# Regular expression to match the prefix pattern\n",
    "prefix_pattern = re.compile(r'^embed-([a-zA-Z0-9]+)_')\n",
    "# Iterate over all files in the specified folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Use the regex to search for the prefix in the filename\n",
    "    match = prefix_pattern.match(filename)\n",
    "    if match:\n",
    "        # If a prefix is found, add it to the set\n",
    "        unique_prefixes.add(match.group(1))\n",
    "\n",
    "# Convert the set to a sorted list if you want the prefixes sorted\n",
    "sorted_prefixes = sorted(list(unique_prefixes))\n",
    "\n",
    "# Print the unique (and possibly sorted) prefixes\n",
    "for prefix in sorted_prefixes:\n",
    "    print(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb36ff62",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest .npy file starting with 'p225' is p225_113 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p226' is p226_009 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p227' is p227_375 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p228' is p228_214 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p229' is p229_168 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p230' is p230_260 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p231' is p231_353 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p232' is p232_311 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p233' is p233_222 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p234' is p234_053 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p236' is p236_122 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p237' is p237_011 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p238' is p238_170 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p239' is p239_422 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p240' is p240_231 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p241' is p241_302 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p243' is p243_273 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p244' is p244_002 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p245' is p245_131 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p246' is p246_173 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p247' is p247_040 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p248' is p248_121 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p249' is p249_012 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p250' is p250_155 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p251' is p251_066 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p252' is p252_024 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p253' is p253_117 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p254' is p254_129 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p255' is p255_255 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p256' is p256_058 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p257' is p257_324 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p258' is p258_183 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p259' is p259_139 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p260' is p260_035 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p261' is p261_106 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p262' is p262_282 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p263' is p263_077 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p264' is p264_206 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p265' is p265_335 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p266' is p266_377 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p267' is p267_182 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p268' is p268_325 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p269' is p269_216 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p270' is p270_209 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p271' is p271_262 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p272' is p272_378 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p273' is p273_313 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p274' is p274_162 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p275' is p275_109 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p276' is p276_013 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p277' is p277_078 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p278' is p278_119 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p279' is p279_172 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p280' is p280_097 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p281' is p281_262 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p282' is p282_220 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p283' is p283_313 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p284' is p284_162 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p285' is p285_397 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p286' is p286_013 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p287' is p287_120 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p288' is p288_041 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p292' is p292_282 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p293' is p293_238 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p294' is p294_049 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p295' is p295_335 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p297' is p297_182 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p298' is p298_325 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p299' is p299_059 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p300' is p300_062 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p301' is p301_151 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p302' is p302_204 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p303' is p303_020 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p304' is p304_251 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p305' is p305_362 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p306' is p306_320 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p307' is p307_213 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p308' is p308_372 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p310' is p310_149 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p311' is p311_235 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p312' is p312_038 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p313' is p313_344 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p314' is p314_135 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p316' is p316_044 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p317' is p317_338 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p318' is p318_259 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p323' is p323_224 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p326' is p326_124 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p329' is p329_045 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p330' is p330_102 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p333' is p333_018 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p334' is p334_269 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p335' is p335_202 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p336' is p336_318 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p339' is p339_279 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p340' is p340_295 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p341' is p341_138 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p343' is p343_098 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p345' is p345_195 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p347' is p347_322 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p351' is p351_304 with a size of -1 elements.\n",
      "The biggest .npy file starting with 'p374' is p374_200 with a size of -1 elements.\n"
     ]
    }
   ],
   "source": [
    "#Get all embeddings!\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Specify the path to the folder containing the .npy files\n",
    "folder_path = '/home/marinjezidzic/Downloads/tf_multispeakerTTS_fc/datasets/vctk/synthesizer/embeds/'\n",
    "\n",
    "# List of prefixes to search for\n",
    "prefixes = sorted_prefixes  # Add or remove prefixes as needed\n",
    "\n",
    "# Initialize a dictionary to store the biggest file for each prefix\n",
    "biggest_files = {prefix: {'filename': None, 'size': -1} for prefix in prefixes}\n",
    "\n",
    "# Iterate over all files in the specified folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an .npy file\n",
    "    if filename.endswith('.npy'):\n",
    "        for prefix in prefixes:\n",
    "            # Check if the filename starts with the current prefix\n",
    "            if filename.startswith('embed-'+prefix):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                \n",
    "                # Load the numpy array\n",
    "                npy_array = np.load(file_path)\n",
    "                \n",
    "                # Check if this array is bigger than the current biggest for this prefix\n",
    "                if npy_array.size > biggest_files[prefix]['size']:\n",
    "                    biggest_files[prefix]['filename'] = filename[6:-4]\n",
    "\n",
    "# Prepare to print the results\n",
    "for prefix, info in biggest_files.items():\n",
    "    if info['filename'] is not None:\n",
    "        print(f\"The biggest .npy file starting with '{prefix}' is {info['filename']} with a size of {info['size']} elements.\")\n",
    "    else:\n",
    "        print(f\"No .npy files starting with '{prefix}' were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace329a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p225_113'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggest_files['p225']['filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6752d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biggest_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b121244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/marinjezidzic/Downloads/dict.json', 'w') as f:\n",
    "    json.dump(biggest_files, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b8d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('/home/marinjezidzic/Downloads/dict.json', 'r') as f:\n",
    "    my_loaded_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "325a0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbbc1050",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Iterator' from 'typing_extensions' (/home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36900/2578393856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/marinjezidzic/Downloads/VoxCeleb_gender/VoxCeleb_gender/males/1.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m transcription = client.audio.transcriptions.create(\n",
      "\u001b[0;32m~/miniconda3/envs/voice_clone/lib/python3.7/site-packages/openai/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotGiven\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProxiesTypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfile_from_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/voice_clone/lib/python3.7/site-packages/openai/types/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from .shared import (\n",
      "\u001b[0;32m~/miniconda3/envs/voice_clone/lib/python3.7/site-packages/openai/types/image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/voice_clone/lib/python3.7/site-packages/openai/_models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mHttpxRequestFiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 36\u001b[0;31m from ._utils import (\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mPropertyInfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mis_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/voice_clone/lib/python3.7/site-packages/openai/_utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mextract_type_var_from_base\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mextract_type_var_from_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_streams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconsume_sync_iterator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconsume_sync_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsume_async_iterator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconsume_async_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from ._transform import (\n\u001b[1;32m     45\u001b[0m     \u001b[0mPropertyInfo\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPropertyInfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/voice_clone/lib/python3.7/site-packages/openai/_utils/_streams.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncIterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconsume_sync_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Iterator' from 'typing_extensions' (/home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file= open(\"/home/marinjezidzic/Downloads/VoxCeleb_gender/VoxCeleb_gender/males/1.wav\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(transcription.text)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "849157c6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (1.16.2)\n",
      "Requirement already satisfied: cached-property in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from openai) (1.5.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from openai) (4.7.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from openai) (3.5.0)\n",
      "Requirement already satisfied: sniffio in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from openai) (0.24.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from openai) (1.10.15)\n",
      "Requirement already satisfied: idna>=2.8 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from httpx<1,>=0.23.0->openai) (0.17.3)\n",
      "Requirement already satisfied: certifi in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/marinjezidzic/miniconda3/envs/voice_clone/lib/python3.7/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc6aa0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
